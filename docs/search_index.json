[["index.html", "1 Welcome to S2Y Lab 2 1.1 Introduction", " S2Y Lab 2 Understanding correlation and further exploring relationships 1 Welcome to S2Y Lab 2 Intended Learning Outcomes: calculate and interpret the sample correlation coefficient; perform hypothesis tests on the population correlation and interpret the decision; calculate least squares estimates of model parameters using vector-matrix formulation. 1.1 Introduction In the lectures we learned how to assess the strength of a linear relationship between random variables using the correlation coefficient. The population correlation is a measure of the magnitude of the strength of the relationship between two random variables \\(X\\) and \\(Y\\), and is defined as \\[\\begin{equation} \\rho(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}, \\tag{1.1} \\end{equation}\\] and can be estimated by replacing each of \\(\\text{Cov}(X,Y)\\), \\(\\text{Var}(X)\\) and \\(\\text{Var}(Y)\\) by their unbiased estimators to give \\[\\begin{equation} r = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}=\\frac{\\sum^{n}_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum^{n}_{i=1}(x_i-\\bar{x})^2(y_i-\\bar{y})^2}}, \\tag{1.2} \\end{equation}\\] the sample correlation coefficient (\\(-1 \\le r \\le 1\\)). Given a sample of data, we can assess the statistical significance of the observed correlations between variables in the wider population. To do this we perform a hypothesis test (more on this in Example 1). "],["example-1-correlation.html", "2 Example 1: Correlation 2.1 Exploratory analysis 2.2 Population correlation hypothesis test 2.3 Calculating the correlation by hand", " 2 Example 1: Correlation In an experiment conducted by the GU physiology department, a sample of volunteers had their power output measured (in watts) while they ran up stairs as fast as possible under different test conditions. Their gender was noted and their weight and leg length measurements recorded. The data are available from the csv file phys1.csv. To open this data file in RStudio type: phys &lt;- read.csv(&quot;phys1.csv&quot;) This will assign the data to the object phys. This worksheet contains six columns, described as follows:              C1: Gender              C2: Weight (kgs)              C3: Leg Length (metres)              C4: Power1: Power output in the stair test              C5: Power2: Power output in a test with a ramp on the stairs              C6: Power3: Power output with ramp on the stairs and a fixed stride length You can view these names by typing names(phys). 2.1 Exploratory analysis QUESTION: Produce a scatterplot to examine the relationship between Power1 and Weight. Solution plot(Power1 ~ Weight, data = phys) Producing labelled scatterplots The simple scatterplot may have ignored the possibility that any relationship between these variables might be different for males and females. So, we now label the plot according to the gender of the subjects. The following provides one approach to doing this in RStudio. The variable Gender within the data is a factor with two levels: Male and Female. Firstly, we will create a character variable for Gender, by using the command: Gender1 &lt;- as.character(phys$Gender) We can now use this new object to label the points by gender. This can be done by typing the following: plot(Power1 ~ Weight, data = phys, xlab = &quot;Weight (kgs)&quot;, ylab = &quot;Power Output (Watts)&quot;, pch = Gender1) which produces the scatterplot given in Figure 2.1. Figure 2.1: Scatterplot of Power1 versus Weight with gender highlighted. DISCUSSION: What can we say about the relationship between Power1 and Weight for males and females separately? The plotting characters (as well as colour) highlighting males and females can also be chosen using numerical characters in R. For example, Gender2 &lt;- as.numeric(phys$Gender) 2.2 Population correlation hypothesis test Here we would like to assess the strength of the relationship, if any, between Power1 and Weight. We are going to do this for male and female volunteers separately, such that we assess the statistical significance of the observed correlation between these two variables in the wider populations of (i) males and (ii) females. Firstly, we subset the data for males and females. One way to do this is: physM &lt;- subset(phys, Gender == &quot;Male&quot;, data = phys) physF &lt;- subset(phys, Gender == &quot;Female&quot;, data = phys) We now perform, separately for males and females, the following hypothesis test: \\(H_0: \\rho=0\\) vs. \\(H_1:\\rho \\ne 0\\) We test the null hypothesis, \\(H_0\\), that is, that in the population of males/females the correlation between Power Output and Weight is 0 against the alternative hypothesis, \\(H_1\\), that the correlation is not equal to 0. To compute the sample correlation coefficient, \\(r\\), and perform our hypothesis test, we use the cor.test command. For the males data, the command is as follows: cor.test(physM$Power1, physM$Weight) Note: in the cor.test command the $ notation is required to access the variables from the subsetted data. The hypothesis test produces a \\(p\\)-value, where we reject the null hypothesis, \\(H_0\\), for small values of the \\(p\\)-value (typically \\(p\\)-values \\(&lt; 0.05\\)). It also produces a 95% confidence interval for a range of plausible values for the true population correlation. QUESTION: What is the sample correlation coefficient for the males data? What is the \\(p\\)-value for the test? The sample correlation coefficient tells us that Power Output and Weight for males have a weakstrong, positivenegative, linearnon-linear relationship. Based on the \\(p\\)-value of the test, we would reject the null hypothesisreject the alternative hypothesisfail to reject the null hypothesisfail to reject the alternative hypothesis and conclude that \\(\\rho\\) is significantly different from zerois not significantly different from zero. Now repeat the hypothesis test for the females data. QUESTION: What is the sample correlation coefficient for the females data? What is the \\(p\\)-value for the test? The sample correlation coefficient tells us that Power Output and Weight for females have a weakstrong, positivenegative, linearnon-linear relationship. Based on the \\(p\\)-value of the test, we would reject the null hypothesisreject the alternative hypothesisfail to reject the null hypothesisfail to reject the alternative hypothesis and conclude that \\(\\rho\\) is significantly different from zerois not significantly different from zero. Note: if we do not subset the data by gender, we obtain a sample correlation coefficient \\(r = 0.89\\), with a \\(p\\)-value \\(\\leq 0.05\\), and thus we would conclude that there is a strong, positive linear relationship between Power Output and Weight. However, we now know that is not the case for females. 2.3 Calculating the correlation by hand To compute the sample correlation coefficient, \\(r\\), the command cor.test uses the formula given in (1.2). To check that the cor.test command is ‘correct’, we can ourselves calculate the correlation directly using the following commands: var # computes the variance of a given vector cov # computes the covariance between the vectors x and y sum # returns the sum of the values given mean # computes the mean of a given vector sqrt # computes the square-root of a given vector TASK 1: Give your answers to 3 decimal places where required. Using the commands var and cov, compute the variance and covariance given in (1.1) for the males data, where \\(Y\\) denotes the response variable Power Output, and \\(X\\) denotes the explanatory variable Weight.        The variance of the explanatory variable Weight is .        The variance of the response variable Power Output is .        The covariance of the response and explanatory variables is . Using the variance and covariance obtained in (a), and the square-root command, sqrt, compute the sample correlation coefficient using the formula given in (1.1) for the males data. Does this match what was obtained using the cor.test command?        The sample correlation coefficient using the variance and covariance obtained in (a) is .        This matchesdoes not match the sample correlation coefficient found using cor.test. Use the mean, sum, and sqrt commands to compute the sum of squares, \\(S_{xx}\\) and \\(S_{yy}\\), and the sum of products, \\(S_{xy}\\), given in formula (1.2) for the males data.        \\(S_{xx}\\):        \\(S_{yy}\\):        \\(S_{xy}\\): Use the sum of squares and sum of products obtained in (c) to compute the sample correlation coefficient, \\(r\\), using formula (1.2) for the males data. Does this match your answer from (b) and the cor.test command?        The sample correlation coefficient found using the sum of squares and sum of products obtained in (c) is . This matchesdoes not match the answer from (b) and the sample correlation coefficient found using cor.test. Repeat (a)–(d) for the females data and compare the answers obtained from formulae (1.1) and (1.2) with that found using the cor.test command. "],["example-2-simulating-random-variables.html", "3 Example 2: Simulating random variables 3.1 Independent random variables 3.2 Correlated random variables", " 3 Example 2: Simulating random variables We can use R to generate random variables. Here we will generate two random variables, \\(X\\) and \\(Y\\), that are, or aren't, related to each other in some form. This can help us understand the relationship between two variables and how they may be correlated. 3.1 Independent random variables Let's start by first generating two random variables that are independent of one another. You have already been introduced to the normal distribution, where some random variable, \\(X\\), is centred at mean \\(\\mu\\), with variance \\(\\sigma^2\\), such that \\(X \\sim N(\\mu, \\sigma^2)\\). To generate random variables from the normal distribution we can use the rnorm command. For example, let's say we want to generate \\(n=30\\) samples from two random variables, \\(X\\) and \\(Y\\), each with mean \\(\\mu = 10\\), and standard deviation \\(\\sigma = 1\\), we can do this as follows: n &lt;- 30 mu &lt;- 10 sigma &lt;- 1 y &lt;- rnorm(n, mean = mu, sd = sigma) x &lt;- rnorm(n, mean = mu, sd = sigma) Note: since we are generating random numbers it is best to use the command set.seed beforehand to be able to replicate the results. For example, if we first run the line set.seed(1), and then the above code, we will generate the same random variables x and y, and a different set if we change the seed value. This gives us our random variables \\(X\\) and \\(Y\\), generated independently of one another, and so we should expect to see no relationship between them. Using set.seed(1) to obtain \\(X\\) and \\(Y\\), a scatterplot of their relationship is shown in Figure 3.1. Figure 3.1: Scatterplot and boxplots of two randomly generated variables \\(X\\) and \\(Y\\). DISCUSSION: What can we say about the relationship between \\(X\\) and \\(Y\\) from Figure 3.1? What can be said about the two boxplots in Figure 3.1? Use the cor.test command to perform a correlation hypothesis test. What does this tell us about the relationship between the two variables? 3.2 Correlated random variables Let us now look at how to generate random variables that are correlated in some way. To do that we will need to obtain the covariance or correlation matrix, and generate the random variables from a multivariate normal distribution. The multivariate normal distribution is a generalisation of the normal distribution to higher dimensions, and as multivariate data analysis is not covered until Honours level we will simplify things a little. To generate correlated random variables we can use the mvrnorm command (load library(MASS)). This requires us to obtain the covariance matrix, \\(\\Sigma\\), explaining the relationship between our two random variables. As you will find out in 2X, the covariance is scale dependent, and as such, the correlation is often easier to use and interpret. We simplify things by generating random variables, \\(X\\) and \\(Y\\), such that their variances are the same and equal to one. Assuming \\(\\text{Var}(X) = \\text{Var}(Y) = 1\\), we can then see from formula (1.1) that \\[\\rho(X,Y) =\\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}} = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{1 \\cdot 1}} = \\mathrm{Cov}(X,Y),\\] that is, the correlation and covariance are the same. This now means that the covariance matrix, \\(\\Sigma\\), and the correlation matrix, \\(P\\), are now equivalent, such that, \\[\\Sigma = \\begin{bmatrix} \\text{Var}(X) &amp; \\text{Cov}(X, Y) \\\\ \\text{Cov}(X, Y) &amp; \\text{Var}(Y) \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\rho(X, Y) \\\\ \\rho(X, Y) &amp; 1 \\end{bmatrix} = P\\] Now, let's say we want to generate two random variables, \\(X\\) and \\(Y\\), from a multivariate normal distribution, such that \\((X, Y) \\sim N(\\boldsymbol{\\mu}, \\Sigma)\\). This can be done using the mvrnorm command as follows: set.seed(1) n &lt;- 30 mu &lt;- c(10, 10) rho &lt;- 0.85 Sigma &lt;- matrix(rho, nrow = 2, ncol = 2) + diag(2) * (1 - rho) rand.vars &lt;- mvrnorm(n, mu = mu, Sigma = Sigma) x &lt;- rand.vars[, 1] y &lt;- rand.vars[, 2] Here we have generated \\(n = 30\\) random samples from a multivariate normal distribution for \\(X\\) and \\(Y\\), with mean vector \\(\\boldsymbol{\\mu}^\\top =\\begin{bmatrix} \\mu_X &amp; \\mu_Y \\end{bmatrix} = \\begin{bmatrix}10 &amp; 10 \\end{bmatrix}\\), and correlation matrix, \\(P\\), such that \\(\\rho(X, Y) = 0.85\\). Figure 3.2: Scatterplot and boxplots of two randomly generated variables \\(X\\) and \\(Y\\). DISCUSSION: What can we say about the relationship between the random variables \\(X\\) and \\(Y\\) from the scatterplot in Figure 3.2? Use the cor.test command to obtain an estimate of the sample correlation coefficient, \\(r\\). Is this the same as the correlation parameter, \\(\\rho\\), used to generate the data? What happens with the sample correlation, \\(r\\), and the `true' correlation, \\(\\rho\\), as the number of samples, \\(n\\), increases? Edit the R code to obtain a weak-to-moderate positive linear relationship between \\(X\\) and \\(Y\\). Use a scatterplot to examine this relationship, and use the cor.test command to see if the sample correlation matches that used to simulate the data. Generate two random variables that exhibit a moderate-to-strong negative linear relationship. Use a plot to display this relationship, and obtain an estimate of the sample correlation coefficient. "],["exercise-1-spurious-correlation.html", "4 Exercise 1: Spurious correlation", " 4 Exercise 1: Spurious correlation Hollywood legend Nicholas Cage seems to have a problem. It appears that every time he releases a new film upon the world many people drown by falling into pools of water. Coincidence? Or, are some of his films that bad? Data: Cage.csv Columns:               C1: Year Year of film releases               C2: NumFilms   Number of Nicholas Cage films released that year               C3: NumDrowns Number of people who drowned falling into pools that year Read in the data using: Cage &lt;- read.csv(&quot;Cage.csv&quot;) TASK 2 Produce a scatterplot of NumDrowned (y) against NumFilms (x). What can we say about their relationship? Use the cor.test command to perform a correlation hypothesis test. What does this tell us about the relationship between NumDrowned and NumFilms? Hint What is the sample correlation coefficient and how to interpret this value? What is \\(p\\)-value associated with the hypothesis test and how to interpret this value? This is an example of spurious correlation, where two variables that are not related to each other in any way, that is, they are independent, could be inferred as being related. The number of Nicholas Cage films released in a year is clearly not related to the number of drowning accidents in that same year, but if we just take our correlation hypothesis test on face value, then we would think otherwise. Reference: Spurious Correlations, Tyler Vigen "],["exercise-2-identifying-relationships.html", "5 Exercise 2: Identifying relationships", " 5 Exercise 2: Identifying relationships For the following contexts determine whether fitting a regression model would be appropriate. If a regression model is appropriate, identify which variable is the response variable and which is the explanatory variable. Is federal spending, on average, higher or lower in countries with high rates of poverty?        Regression model appropriate? YesNo        Federal spending: ExplanatoryResponseN/A        Poverty rates: ExplanatoryResponseN/A A study was conducted to determine whether surgery or chemotherapy results in higher survival rates for a certain type of cancer.        Regression model appropriate? YesNo        Type of treatment: ExplanatoryResponseN/A        Survival rates: ExplanatoryResponseN/A A study found that, overall, left-handed people die at a younger age than right-handed people.        Regression model appropriate? YesNo        Age of death: ExplanatoryResponseN/A        Left- or right-handed: ExplanatoryResponseN/A Per capita cheese consumption is correlated with the number of people who died getting tangled in bed sheets.        Regression model appropriate? YesNo        Number of people who died getting tangled in bed sheets: ExplanatoryResponseN/A        Per capita cheese consumption: ExplanatoryResponseN/A An experiment was conducted to test the effects of sleep deprivation on human reaction times.        Regression model appropriate? YesNo        Hours of sleep: ExplanatoryResponseN/A        Reaction times: ExplanatoryResponseN/A A study was conducted in order to predict the GPA of university students given their high school GPA.        Regression model appropriate? YesNo        GPA of university students: ExplanatoryResponseN/A        High school GPA: ExplanatoryResponseN/A A company wants to know if there is a significant relationship between its advertising expenditures and its sales volume.        Regression model appropriate? YesNo        Advertising expenditures: ExplanatoryResponseN/A        Sales volume: ExplanatoryResponseN/A A sample of insured drivers with similar insurance policies were randomly selected. Interest is in determining whether there is a significant relationship between driving experience and insurance premium.        Regression model appropriate? YesNo        Driving experience: ExplanatoryResponseN/A        Insurance premium: ExplanatoryResponseN/A Ice cream sales are correlated with murder rates in the US.        Regression model appropriate? YesNo        Murder rates: ExplanatoryResponseN/A        Ice cream sales: ExplanatoryResponseN/A "],["solution.html", "6 Solution 6.1 Task 1 6.2 Task 2", " 6 Solution 6.1 Task 1 X &lt;- physM$Weight Y &lt;- physM$Power1 c(var(X), var(Y), cov(X, Y)) ## [1] 47.37381 25435.39226 893.14684 cov(X, Y)/sqrt(var(X)*var(Y)) ## [1] 0.8136441 cor.test(physM$Weight, physM$Power1)$estimate ## cor ## 0.8136441 Sxx &lt;- sum((X-mean(X))^2) Syy &lt;- sum((Y-mean(Y))^2) Sxy &lt;- sum((X-mean(X))*(Y-mean(Y))) c(Sxx, Syy, Sxy) ## [1] 663.2333 356095.4916 12504.0557 Sxy/sqrt(Sxx*Syy) ## [1] 0.8136441 X &lt;- physF$Power1 Y &lt;- physF$Weight cor.test(X, Y) ## ## Pearson&#39;s product-moment correlation ## ## data: X and Y ## t = 1.42, df = 13, p-value = 0.1791 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.1795155 0.7398300 ## sample estimates: ## cor ## 0.3664454 cov(X, Y)/sqrt(var(X)*var(Y)) ## [1] 0.3664454 Sxx &lt;- sum((X-mean(X))^2) Syy &lt;- sum((Y-mean(Y))^2) Sxy &lt;- sum((X-mean(X))*(Y-mean(Y))) Sxy / sqrt(Sxx * Syy) ## [1] 0.3664454 6.2 Task 2 Producing a scatterplot plot(NumDrowned ~ NumFilms, data = Cage, xlab = &quot;Number of Nicholas Cage films released in a year&quot;, ylab = &quot;Number of people who drowned falling into pools that year&quot;) Correlation test cor.test(Cage$NumDrowned, Cage$NumFilms) ## ## Pearson&#39;s product-moment correlation ## ## data: Cage$NumDrowned and Cage$NumFilms ## t = 2.6785, df = 9, p-value = 0.02527 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1101273 0.9045101 ## sample estimates: ## cor ## 0.6660043 The sample correlation coefficient is 0.666, indicating a moderate positive linear relationship between NumDrowned and NumFilms. The \\(p\\)-value is less than 0.05. Therefore, we reject the null hypothesis and conclude that the population correlation \\(\\rho\\) is statistically significantly different from 0. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
